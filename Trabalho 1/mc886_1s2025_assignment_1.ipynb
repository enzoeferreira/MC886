{"cells":[{"cell_type":"markdown","metadata":{"id":"NGk_Hx6O5Xw7"},"source":["### **State University of Campinas - UNICAMP** </br>\n","**Course**: MC886A </br>\n","**Professor**: Marcelo da Silva Reis </br>\n","**TA (PED)**: Marcos Vinicius Souza Freire\n","\n","---"]},{"cell_type":"markdown","source":["**Student 1**:\n","\n","**RA 1**:\n","\n","**Student 2**:\n","\n","**RA 2**:"],"metadata":{"id":"9ladvjScB0B7"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"Vt1kV57fCQ34"}},{"cell_type":"markdown","metadata":{"id":"gT-27XAf5XxG"},"source":["### **Assignment 1: MC886A**\n","##### Notebook: mc886_1s2025-assignment_1.ipynb"]},{"cell_type":"markdown","metadata":{"id":"ugbXcm3i5XxH"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"P6i80tpG5XxI"},"source":["Dear Students, welcome to MC886!!!\n","\n","**You've Got This!** ðŸ¤—  \n","\n","Now that you are building a strong foundation in regression and classification through the first lectures, it's time to put your skills into action with this notebook! Remember, the goal isn't just to \"get the code running\" but to **deeply understand** how these algorithms work under the hood.  \n","\n","- **For the Linear Regression section:**  \n","  Implementing the Linear Regressor manually might feel tricky at first, but trust the process. Every line of code you write, from initializing weights to updating gradients, is a step toward mastering the core of machine learning. Compare your results with PyTorch's implementation to see how theory translates into powerful frameworks!  \n","\n","- **For the KNN Classifier:**  \n","  Building a classifier from scratch will test your problem-solving skills. Donâ€™t worry if your first attempt isn't perfect, debugging is part of the journey!  \n","\n","**Pro Tips:**  \n","1. Use the visualizations and mutual information analysis to guide your feature engineering.  \n","2. When stuck, revisit the math behind MSE, MAE, and RÂ². They're not just metrics, but tools to evaluate your model's \"story\". Also, remember to discuss the implementation with your pair. Don't hesitate in reaching the TA (\"PED\") via Discord (https://discord.gg/ArdB8FtBcq).   \n","3. Celebrate small wins! Got your manual linear regressor to train? That's a victory.  \n","\n","**Remember:**  \n","Machine learning is a marathon, not a sprint. Struggling with a concept? That's where the real learning happens. You're not just writing code, you're training your brain to think like a data scientist.  \n","\n","Let curiosity drive you, and don't hesitate to ask questions.\n","\n","ðŸŒ… **Go make those models shine!** ðŸš€  "]},{"cell_type":"markdown","metadata":{"id":"jRK7rqaE5XxL"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Ph95XRdn5XxM"},"source":["### **Table of Contents**\n","\n","1. [**Objective**](#objective) </br>\n","2. [**Linear Regression**](#linear-regression) </br>\n","3. [**Data analysis and preprocessing**](#data-analysis-and-preprocessing) </br>\n","4. [**Implement and train a Linear Regressor**](#implement-and-train-a-linear-regressor) </br>\n","5. [**Compare with PyTorch Linear Regression**](#compare-with-pytorch-linear-regression) </br>\n","6. [**Find interaction terms**](#find-interaction-terms) </br>\n","7. [**K-Nearest Neighbors (KNN) Classifier**](#k-nearest-neighbors-knn-classifier) </br>\n","8. [**Data analysis and preprocessing**](#data-analysis-and-preprocessing) </br>\n","9. [**Train a K-Nearest Neighbors Classifier**](#train-a-k-nearest-neighbors-classifier) </br>\n","10. [**Multiclass Classification**](#multiclass-classification) </br>\n","11. [**REFERENCES**](#references)\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"M33BcjVy5XxN"},"source":["##### **Objective:**\n","\n","To explore **Linear Regression** and **K-Nearest Neighbors** alternatives and come up with the best possible model for the problems. In this work, we will train two models for regression (one manual and one using PyTorch) and one for multiclass classification using PyTorch.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"rUisX9H65XxO"},"source":["#### **Linear Regression**\n","\n","In this section, you must load and explore the dataset, and build a linear regressor by hand. **No machine learning libraries are allowed for this implementation**. After building your own regressor, you must compare it with a PyTorch-based linear regression model.\n","\n","##### **Dataset: Life Expectancy Data (Rajarshi *et al.*, 2018)[1]**\n","\n","This dataset contains health and socio-economic factors collected across various countries over multiple years, with the goal of predicting life expectancy.\n","\n","Understanding life expectancy helps countries to allocate resources effectively and identify key factors influencing public health. This dataset was collected to study how various health, economic, and social indicators impact life expectancy.\n","\n","Features and their descriptions:\n","- **Year**: Year of the sample\n","- **Status**: Development status of the country (Developed/Developing)\n","- **Life expectancy (TARGET)**: Average life expectancy at birth (in years)\n","- **Adult Mortality**: Probability of dying between 15 and 60 years per 1000 population\n","- **infant deaths**: Number of infant deaths per 1000 population\n","- **Alcohol**: Per capita alcohol consumption (in liters of pure alcohol)\n","- **percentage expenditure**: Expenditure on health as a percentage of GDP\n","- **Hepatitis B**: Hepatitis B immunization coverage among 1-year-olds (%)\n","- **Measles**: Number of reported measles cases per 1000 population\n","- **BMI**: Average Body Mass Index of the population\n","- **under-five deaths**: Number of under-five deaths per 1000 population\n","- **Polio**: Polio immunization coverage among 1-year-olds (%)\n","- **Total expenditure**: General government expenditure on health as a percentage of total government expenditure\n","- **Diphtheria**: Diphtheria immunization coverage among 1-year-olds (%)\n","- **HIV/AIDS**: Deaths per 1000 live births due to HIV/AIDS (0-4 years)\n","- **GDP**: Gross Domestic Product per capita (in USD)\n","- **Population**: Population of the country\n","- **thinness 1-19 years**: Prevalence of thinness among children and adolescents aged 1-19 (%)\n","- **thinness 5-9 years**: Prevalence of thinness among children aged 5-9 (%)\n","- **Income composition of resources**: Human Development Index component for income (0-1 scale)\n","- **Schooling**: Average number of years of schooling\n","\n","**How to load the dataset**\n","\n","- For this task, we will load the dataset from a local CSV file. Ensure `life_expectancy_data.csv` is in your working directory or adjust the path accordingly.\n","- The dataset is located in `assignement_1/data/life_expectancy_data.csv`"]},{"cell_type":"markdown","metadata":{"id":"tGLPac165XxP"},"source":["---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CP969eDo5XxQ"},"outputs":[],"source":["# Import necessary libraries for data handling, visualization, and PyTorch-based modeling\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torchmetrics import MeanSquaredError, MeanAbsoluteError, R2Score, Accuracy, F1Score, ConfusionMatrix\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvUZ5RC05XxT"},"outputs":[],"source":["### REMEMBER TO RE-VISIT THE NOTEBOOKS \"00-setup.ipynb\" and \"01-pytorch-tensor_basics.ipynb\", from our first hands-on, so you can set the right environment for you."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_MtMN825XxU"},"outputs":[],"source":["# Load dataset from a local CSV file\n","# Ensure the file \"Life Expectancy Data.csv\" is in the \"data\" folder within your working directory (RE-VISIT THE NOTEBOOKS \"00-setup.ipynb\" and \"01-pytorch-tensor_basics.ipynb\")\n","df = pd.read_csv(\"MC886_1S2025/assignment_1/data/life_expectancy_data.csv\")  # You may change this path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUaM-Gj_5XxV"},"outputs":[],"source":["# Drop non-numeric or unnecessary columns\n","df = df.drop([\"Country\"], axis=1)  # Country names aren't directly usable\n","\n","# Convert the categorical \"Status\" column (Developed/Developing) into numeric values\n","# Developed = 1, Developing = 0 (binary encoding)\n","df[\"Status\"] = df[\"Status\"].map({\"Developed\": 1, \"Developing\": 0})\n","\n","# Handle missing values (fill with median for simplicity)\n","df = df.fillna(df.median())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pii_xtOf5XxV"},"outputs":[],"source":["# Define features (X) and target (y) for the regression task\n","# X: all columns except \"Life expectancy\" (the target)\n","# y: \"Life expectancy\" column, reshaped to a column vector for compatibility\n","X = torch.tensor(df.drop(\"Life expectancy\", axis=1).values, dtype=torch.float32)\n","y = torch.tensor(df[\"Life expectancy\"].values, dtype=torch.float32).view(-1, 1)\n","\n","# Split the data into training and testing sets\n","train_size = int(0.8 * len(X)) # Example using 80%\n","X_train, X_test = X[:train_size], X[train_size:]\n","y_train, y_test = y[:train_size], y[train_size:]\n","\n","# Compute normalization parameters using only the training data\n","X_train_mean = X_train.mean(dim=0)\n","X_train_std = X_train.std(dim=0)\n","y_train_mean = y_train.mean()\n","y_train_std = y_train.std()\n","\n","# Normalize training and testing data using training parameters\n","X_train = (X_train - X_train_mean) / X_train_std\n","X_test = (X_test - X_train_mean) / X_train_std  # Use training mean and std\n","y_train = (y_train - y_train_mean) / y_train_std\n","y_test = (y_test - y_train_mean) / y_train_std  # Use training mean and std"]},{"cell_type":"markdown","metadata":{"id":"7aSUf7xK5XxW"},"source":["#### **Data analysis and preprocessing**\n","\n","$({1.5} \\space point)$\n","\n","Explore the dataset by plotting graphs of features you think are important to visualize their relationship with the target (`Life expectancy`). Use boxplots to understand feature distributions. There are no minimum/maximum requirements for graphs. Explore what helps you understand the dataset.\n","\n","Check the dependencies between features and the target to identify which have the biggest impact (see the `mutual_information` section below).\n","\n","The dataset has one categorical feature (`Status`) that has been encoded as binary (1=Developed, 0=Developing). Ensure all features are numeric and handle missing values appropriately.\n","\n","Machine learning models are sensitive to the scale of input features, so normalization is applied above.\n","\n","**Mutual Information**\n","\n","Mutual information measures the dependency between variables and can help identify features strongly associated with `Life expectancy`. For this task, you may use [`mutual_info_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html) from Scikit-learn as an exploratory tool (an exception to the no-Scikit-learn rule for modeling)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"71xkXweQ5XxW"},"outputs":[],"source":["import numpy as np\n","import plotly.express as px # Here I proposed Plotly, for interactive plots. You can use other libraries, such as Seaborn or Matplotlib, if you prefer.\n","from sklearn.feature_selection import mutual_info_regression\n","\n","# Prepare raw data (before normalization) for mutual information calculation\n","X_raw = df.drop(\"Life expectancy\", axis=1)\n","y_raw = df[\"Life expectancy\"]\n","\n","# Compute mutual information; \"Year\" (index 0)\n","mt_info = mutual_info_regression(X_raw, y_raw, discrete_features=[0])  # Year is is treated as discrete\n","\n","# Create a Series for visualization\n","mt_info_df = pd.Series(mt_info, index=X_raw.columns)\n","\n","# Plot mutual information as a bar chart using Plotly or other library of your choice\n","fig = px.bar(\n","    x=mt_info_df.index,\n","    y=mt_info_df.values,\n","    labels={'x': 'Features', 'y': 'Mutual Information'},\n","    title='Feature Importance',\n","    height=400,\n","    width=800\n",")\n","fig.update_layout(\n","    xaxis_tickangle=-45,\n","    xaxis_title=\"Features\",\n","    yaxis_title=\"Mutual Information\",\n","    bargap=0.2\n",")\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"Q1H8ctFi5XxX"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"_XePH56X5XxX"},"source":["##### **Discussion of key points**\n","- How did the visualization help in understanding the data?\n","- Looking at the mutual information plot, can you find features that seem uninfluential? (If so, consider removing them before proceeding!)"]},{"cell_type":"markdown","metadata":{"id":"h4M3qRBi5XxY"},"source":["*YOUR ANSWER HERE*"]},{"cell_type":"markdown","metadata":{"id":"G5agLl-x5XxY"},"source":["##### **Implement and train a Linear Regressor**\n","\n","$({2.5} \\space points)$\n","\n","Complete the implementation of the `MyLinearRegressor` class and the `MSE` metric below. No machine learning libraries are allowed for these.\n","\n","Evaluate performance using Mean Squared Error ($MSE$), Mean Absolute Error ($MAE$), and Coefficient of Determination ($R^{2}$) from [`torchmetrics`](https://lightning.ai/docs/torchmetrics/stable/).\n","\n","*P.S.: You don't need to build anything using Lightning to implement the metrics from TorchMetrics.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A76LwPnu5XxY"},"outputs":[],"source":["# TASK: MSE. You cannot use machine learning libraries for this!!\n","# Mean Squared Error (MSE)\n","def MSE(y_true, y_pred):\n","    return torch.mean((y_true - y_pred) ** 2)\n","\n","# TASK: MAE. You cannot use machine learning libraries for this!!\n","# Mean Absolute Error (MAE)\n","\n","# TASK: R^2. You cannot use machine learning libraries for this!!\n","# Coefficient of Determination (R^2)\n","\n","# Custom Linear Regression class (manual implementation)\n","class MyLinearRegressor:\n","    def __init__(self, learning_rate=0.01, max_iter=1000):\n","        self.max_iter = max_iter\n","        self.learning_rate = learning_rate\n","        self.weights = None\n","        self.bias = None\n","\n","    def fit(self, X, y):\n","        # Initialize weights and bias to zeros\n","        n_samples, n_features = X.shape\n","        self.weights = torch.zeros(n_features, 1, dtype=torch.float32)\n","        self.bias = torch.zeros(1, dtype=torch.float32)\n","\n","        # Gradient descent loop\n","        for _ in range(self.max_iter):\n","            y_pred = self.predict(X)\n","            grad_w = (2 / n_samples) * X.T @ (y_pred - y)\n","            grad_b = (2 / n_samples) * torch.sum(y_pred - y)\n","            # Update weights and bias by stepping in the opposite direction of the gradient\n","            self.weights -= self.learning_rate * grad_w\n","            self.bias -= self.learning_rate * grad_b\n","\n","    def predict(self, X):\n","        # Linear equation: y = X * weights + bias\n","        return X @ self.weights + self.bias\n","\n","# Train the manual regressor on the training data\n","regressor = MyLinearRegressor(learning_rate=0.01, max_iter=1000)\n","regressor.fit(X_train, y_train)\n","y_pred_manual = regressor.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"8P9cIBfB5XxZ"},"source":["#### **Discussion of key points**\n","- Which of the proposed metrics $(MSE, MAE, R^{2})$ is best for this problem? Why?\n","- Did your Linear Regressor accurately estimate life expectancy? Justify using a metric.\n","- What do you think is the biggest error type in your model: variance or bias?"]},{"cell_type":"markdown","metadata":{"id":"hS-sNUpv5XxZ"},"source":["*YOUR ANSWER HERE*"]},{"cell_type":"markdown","metadata":{"id":"Zc4VmYwT5Xxa"},"source":["##### **Compare with PyTorch Linear Regression**\n","\n","$({0.5} \\space point)$\n","\n","Train a PyTorch-based linear regression model and compare it with your manual implementation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgazDLOQ5Xxa"},"outputs":[],"source":["# PyTorch-based Linear Regression class\n","class LinearRegression(nn.Module):\n","    def __init__(self, input_dim):\n","        super(LinearRegression, self).__init__()\n","        self.linear = nn.Linear(input_dim, 1)\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","# Initialize model\n","model = LinearRegression(X.shape[1])\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Stochastic Gradient Descent\n","criterion = nn.MSELoss() # Mean Squared Error loss function\n","\n","# Training loop for PyTorch model\n","for epoch in range(1000):\n","    optimizer.zero_grad()\n","    outputs = model(X_train)\n","    loss = criterion(outputs, y_train)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predict with the trained PyTorch model (no gradient tracking)\n","with torch.no_grad():\n","    y_pred_pytorch = model(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XyVkGbE5Xxb"},"outputs":[],"source":["# Define evaluation metrics from torchmetrics for comparison\n","mse = MeanSquaredError()\n","mae = MeanAbsoluteError()\n","r2 = R2Score()\n","\n","# Evaluate the manual regressor\n","mse_manual = mse(y_pred_manual, y_test)\n","mae_manual = mae(y_pred_manual, y_test)\n","r2_manual = r2(y_pred_manual, y_test)\n","\n","# Evaluate the PyTorch model\n","mse_pytorch = mse(y_pred_pytorch, y_test)\n","mae_pytorch = mae(y_pred_pytorch, y_test)\n","r2_pytorch = r2(y_pred_pytorch, y_test)\n","\n","# Print results for comparison\n","print(f\"Manual - MSE: {mse_manual:.4f}, MAE: {mae_manual:.4f}, R2: {r2_manual:.4f}\")\n","print(f\"PyTorch - MSE: {mse_pytorch:.4f}, MAE: {mae_pytorch:.4f}, R2: {r2_pytorch:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"wwKbay9F5Xxb"},"source":["#### **Find interaction terms**\n","\n","$({0.5} \\space point)$\n","\n","Search for interactions between columns (e.g., multiply `Alcohol` and `BMI`) that might improve the model. Use [`mutual_info_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html) to test if new features have higher mutual information with the target. Retrain your manual regressor with these features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rglen5HN5Xxc"},"outputs":[],"source":["# Example: Adding an interaction term to improve the model\n","\n","\n","# Split the DataFrame into train and test\n","train_size = int(0.8 * len(df)) # Example using 80%\n","df_train = df.iloc[:train_size].copy()\n","df_test = df.iloc[train_size:].copy()\n","\n","# Create interaction terms for both sets: with a new feature by multiplying \"Alcohol\" and \"BMI\" (captures combined effect)\n","df_train[\"Alcohol_BMI\"] = df_train[\"Alcohol\"] * df_train[\"BMI\"]\n","df_test[\"Alcohol_BMI\"] = df_test[\"Alcohol\"] * df_test[\"BMI\"]\n","\n","# Define features and target\n","X_train_extra = torch.tensor(df_train.drop(\"Life expectancy\", axis=1).values, dtype=torch.float32)\n","X_test_extra = torch.tensor(df_test.drop(\"Life expectancy\", axis=1).values, dtype=torch.float32)\n","y_train = torch.tensor(df_train[\"Life expectancy\"].values, dtype=torch.float32).view(-1, 1)\n","y_test = torch.tensor(df_test[\"Life expectancy\"].values, dtype=torch.float32).view(-1, 1)\n","\n","# Compute normalization parameters from training data only\n","X_train_mean = X_train_extra.mean(dim=0)\n","X_train_std = X_train_extra.std(dim=0)\n","y_train_mean = y_train.mean()\n","y_train_std = y_train.std()\n","\n","# Normalize using training parameters\n","X_train_extra = (X_train_extra - X_train_mean) / X_train_std\n","X_test_extra = (X_test_extra - X_train_mean) / X_train_std\n","y_train = (y_train - y_train_mean) / y_train_std\n","y_test = (y_test - y_train_mean) / y_train_std\n","\n","# Retrain manual regressor with extra feature\n","regressor_extra = MyLinearRegressor(learning_rate=0.01, max_iter=1000)\n","regressor_extra.fit(X_train_extra, y_train)\n","y_pred_extra = regressor_extra.predict(X_test_extra)\n","\n","# Evaluate the model with the interaction term\n","mse_extra = mse(y_pred_extra, y_test)\n","print(f\"With Interaction - MSE: {mse_extra:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"5RwygiGJ5Xxc"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"TCjyoSBa5Xxd"},"source":["#### **K-Nearest Neighbors (KNN) Classifier**\n","\n","In this section, you must load and explore the dataset, then train a K-Nearest Neighbors (KNN) classifier using PyTorch."]},{"cell_type":"markdown","metadata":{"id":"p5qHmmR05Xxd"},"source":["#### **Dataset: Wine Quality (Cortez *et al*., 2009)[2]**\n","\n","This dataset contains chemical properties of red wine samples, with a quality score assigned by experts. Weâ€™ll adapt it for multiclass classification by binning the quality scores.\n","\n","The goal is to classify wine quality based on its chemical composition, which can assist winemakers in quality control.\n","\n","Features and their descriptions:\n","- **fixed acidity**: Fixed acidity level (g/dmÂ³)\n","- **volatile acidity**: Volatile acidity level (g/dmÂ³)\n","- **citric acid**: Citric acid level (g/dmÂ³)\n","- **residual sugar**: Residual sugar level (g/dmÂ³)\n","- **chlorides**: Chloride level (g/dmÂ³)\n","- **free sulfur dioxide**: Free sulfur dioxide level (mg/dmÂ³)\n","- **total sulfur dioxide**: Total sulfur dioxide level (mg/dmÂ³)\n","- **density**: Density of the wine (g/cmÂ³)\n","- **pH**: pH level\n","- **sulphates**: Sulphate level (g/dmÂ³)\n","- **alcohol**: Alcohol content (% by volume)\n","- **quality (TARGET)**: Quality score (0-10, binned into 3 classes: Low [0-4], Medium [5-7], High [8-10])\n","\n","**How to load the dataset**\n","\n","- Load the dataset from a local CSV file (`wine_quality_red.csv`), which uses semicolons (`;`) as separators.\n","- The dataset is located in `assignment_1/data/wine_quality_red.csv`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZZWx29t5Xxd"},"outputs":[],"source":["# Load the wine quality dataset from a local CSV file\n","# Note: This CSV uses semicolons (;) as separators\n","df = # ...  You may change this path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PHGZNsC85Xxe"},"outputs":[],"source":["# Check column names (optional, for verification)\n","print(df.columns)  # Should print: ['fixed acidity', 'volatile acidity', ..., 'quality']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSwmCagM5Xxe"},"outputs":[],"source":["# Define features (X) and target (y) for classification\n","# X: all columns except \"quality\" (the target)\n","X = torch.tensor(df.drop(\"quality\", axis=1).values, dtype=torch.float32)\n","# Bin \"quality\" into 3 classes: Low (0-4), Medium (5-7), High (8-10)\n","y = torch.tensor(pd.cut(df[\"quality\"], bins=[0, 4, 7, 10], labels=[0, 1, 2]).cat.codes, dtype=torch.long)\n","\n","# Normalize features\n","X = (X - X.mean(dim=0)) / X.std(dim=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXSrnQRT5Xxf"},"outputs":[],"source":["# Split into training (80%) and testing (20%) sets\n","# You can try different splits\n","train_size = int(0.8 * len(X))\n","X_train, X_test = X[:train_size], X[train_size:]\n","y_train, y_test = y[:train_size], y[train_size:]"]},{"cell_type":"markdown","metadata":{"id":"6Xl5DzC35Xxf"},"source":["##### **Data analysis and preprocessing**\n","\n","$({1.5} \\space points)$\n","\n","Explore the dataset as in the Linear Regression section. Use [`mutual_info_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) from Scikit-learn (exploratory exception) to assess feature importance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHwCSEJd5Xxf"},"outputs":[],"source":["from sklearn.feature_selection import mutual_info_classif\n","\n","mt_info = mutual_info_classif(df.drop(\"quality\", axis=1), df[\"quality\"])\n","mt_info_df = pd.Series(mt_info, index=df.drop(\"quality\", axis=1).columns)\n","# (...)\n","# fig.show() or plt.show()? Your call!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9R1dcDZU5Xxg"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"aKb2r3Uu5Xxg"},"source":["Check for missing values and handle them if present (this dataset typically has none).\n","\n","#### **Discussion of key points**\n","- Were there missing values? How did you handle them?"]},{"cell_type":"markdown","metadata":{"id":"u9n5AsXl5Xxp"},"source":["#### **Train a K-Nearest Neighbors Classifier**\n","\n","$({2.5} \\space point)$\n","\n","Implement and train a KNN classifier using PyTorch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vIEvemVM5Xxq"},"outputs":[],"source":["# KNN Classifier\n","class KNNClassifier:\n","    def __init__(self, k=5):\n","        self.k = k # Number of nearest neighbors to consider\n","\n","    def fit(self, X, y):\n","        # Store training data (KNN is a lazy learnerâ€”no real \"training\" happens here)\n","        self.X_train = X\n","        self.y_train = y\n","\n","    def predict(self, X):\n","        # Compute Euclidean distances between test points and training points\n","        distances = torch.cdist(X, self.X_train)\n","        # Get indices of k nearest neighbors (smallest distances)\n","        _, indices = distances.topk(self.k, largest=False)\n","        # Retrieve labels of the k nearest neighbors\n","        k_nearest_labels = self.y_train[indices]\n","        # Predict the most common class (mode) among the k neighbors\n","        predictions = torch.mode(k_nearest_labels, dim=1)[0]\n","        return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VlO94t2j5Xxq"},"outputs":[],"source":["# Train and predict with KNN\n","knn = KNNClassifier(k=5)\n","knn.fit(X_train, y_train)\n","y_pred = knn.predict(X_test)\n","\n","# Metrics with Torchmetrics\n","accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n","f1 = F1Score(task=\"multiclass\", num_classes=3, average=\"macro\")\n","conf_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=3)\n","\n","# Evaluate the KNN classifier\n","acc = accuracy(y_pred, y_test)\n","f1_score = f1(y_pred, y_test)\n","cm = conf_matrix(y_pred, y_test)\n","\n","print(f\"Accuracy: {acc:.4f}, F1-Score: {f1_score:.4f}\")\n","print(\"Confusion Matrix:\\n\", cm)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(6, 6))\n","plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n","plt.title(\"Confusion Matrix\")\n","plt.colorbar()\n","plt.xticks([0, 1, 2], [\"Low\", \"Medium\", \"High\"])\n","plt.yticks([0, 1, 2], [\"Low\", \"Medium\", \"High\"])\n","plt.xlabel(\"True\")\n","plt.ylabel(\"Predicted\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"FKEQ1HeB5Xxr"},"source":["##### **Discussion of key points**\n","- Is accuracy a good metric for this problem? Justify.\n","- What conclusions can you draw from the confusion matrix?\n","- What was the best K? How does K impact the bias-variance tradeoff?"]},{"cell_type":"markdown","metadata":{"id":"iHktUE3M5Xxr"},"source":["*YOUR ANSWER HERE*"]},{"cell_type":"markdown","metadata":{"id":"yuiP9nAf5Xxs"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"gZtNqHh_5Xxs"},"source":["#### **Multiclass Classification**\n","\n","$({1} \\space point)$\n","\n","Adapt the **Life Expectancy Data** target into 4 classes (quartiles) and train a KNN classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEjyLefC5Xxs"},"outputs":[],"source":["# Reload the Life Expectancy dataset for a classification task\n","df = pd.read_csv(\"MC886_1S2025/assignment_1/data/life_expectancy_data.csv\") # You may change this path\n","\n","df = df.drop([\"Country\"], axis=1)\n","df[\"Status\"] = df[\"Status\"].map({\"Developed\": 1, \"Developing\": 0})\n","df = df.fillna(df.median())\n","\n","# Define features and target; bin \"Life expectancy\" into 4 quartiles (Q1-Q4)\n","X = torch.tensor(df.drop(\"Life expectancy\", axis=1).values, dtype=torch.float32)\n","y = torch.tensor(pd.qcut(df[\"Life expectancy\"], q=4, labels=[0, 1, 2, 3]).cat.codes, dtype=torch.long)\n","\n","# Normalize features\n","X = (X - X.mean(dim=0)) / X.std(dim=0)\n","train_size = int(0.8 * len(X))\n","X_train, X_test = X[:train_size], X[train_size:]\n","y_train, y_test = y[:train_size], y[train_size:]\n","\n","# Train and predict with KNN for Life Expectancy classification\n","knn = KNNClassifier(k=5)\n","knn.fit(X_train, y_train)\n","y_pred = knn.predict(X_test)\n","\n","# Define metrics for 4-class classification\n","accuracy = Accuracy(task=\"multiclass\", num_classes=4)\n","f1 = F1Score(task=\"multiclass\", num_classes=4, average=\"macro\")\n","conf_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=4)\n","\n","# Evaluate the classifier\n","acc = accuracy(y_pred, y_test)\n","f1_score = f1(y_pred, y_test)\n","cm = conf_matrix(y_pred, y_test)\n","\n","print(f\"Accuracy: {acc:.4f}, F1-Score: {f1_score:.4f}\")\n","print(\"Confusion Matrix:\\n\", cm)\n","\n","# Visualize the confusion matrix for quartiles\n","plt.figure(figsize=(6, 6))\n","plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n","plt.title(\"Confusion Matrix\")\n","plt.colorbar()\n","plt.xticks([0, 1, 2, 3], [\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n","plt.yticks([0, 1, 2, 3], [\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n","plt.xlabel(\"True\")\n","plt.ylabel(\"Predicted\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Z6JZeefI5Xxt"},"source":["##### **Discussion of key points**\n","- Is accuracy a good metric for this problem? Justify.\n","- What conclusions can you draw from the confusion matrix?\n","- Is there value in solving a regression problem as multiclass classification?"]},{"cell_type":"markdown","metadata":{"id":"mPXfiKbq5Xxt"},"source":["*YOUR ANSWER HERE*"]},{"cell_type":"markdown","metadata":{"id":"Ux_gyUP75Xxt"},"source":["Best of luck!!!"]},{"cell_type":"markdown","source":["##### **Assignment submission**\n","\n","This notebook must be filled with your solution and submitted on the Assignment's entry at our Google Classroom page. Only one member of the pair should submit the solution.\n","\n","##### **Policy for late submissions**\n","\n","You are NOT encouraged to submit the solution after the deadline; however, in this case, the following penalties hold:\n","\n","* 25% of the grade for 1-day late submission;\n","* 50% of the grade for 2-day late submission;\n","* 75% of the grade for 3-day late submission.\n","\n"],"metadata":{"id":"q9WVY5ICOCRK"}},{"cell_type":"markdown","metadata":{"id":"ATlohvJd5Xxu"},"source":["#### **REFERENCES**\n","\n","[1] Rajarshi, K., Russell, D., & Wang, D. (2009). Life Expectancy (WHO). Kaggle. [https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who](https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who).\n","\n","[2] Cortez, P., Cerdeira, A., Almeida, F., Matos, T., & Reis, J. (2009). Wine Quality [Dataset]. UCI Machine Learning Repository. [https://doi.org/10.24432/C56S3T](https://doi.org/10.24432/C56S3T)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}