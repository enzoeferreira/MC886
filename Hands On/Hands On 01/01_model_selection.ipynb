{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_HSgEpR5-3x"
      },
      "source": [
        "### **State University of Campinas - UNICAMP** </br>\n",
        "**Course**: MC886A </br>\n",
        "**Professor**: Marcelo da Silva Reis </br>\n",
        "**TA (PED)**: Marcos Vinicius Souza Freire\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFUBcOBT5-32"
      },
      "source": [
        "### **Hands-On: Model Selection**\n",
        "##### Notebook: 01 Model Selection\n",
        "\n",
        "> Dataset from Scikit Learn - [load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html), based on [Breast Cancer Wisconsin (Diagnostic)](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)(1993)[1]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYQbko5s5-33"
      },
      "source": [
        "**This notebook covers the following topics:**\n",
        "\n",
        "- **Model Selection and Regularization:** Using subset selection (RFE), Ridge (L2) and Lasso (L1) regression.\n",
        "- **Advanced Model Selection:** Applying regularization with PyTorch for logistic regression, and a demonstration with k-Nearest Neighbors and Random Forest.\n",
        "\n",
        "Throughout the notebook we illustrate the methods using formulas, interactive Plotly graphs for the decision boundaries, and well-structured code cells.\n",
        "\n",
        "Based on the Jurafsky & Martin (2025) lectures [2]\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPViT5TV5-34"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Replace Matplotlib with Plotly for interactive plotting\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.datasets import make_classification, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdR1ViJB5-37"
      },
      "source": [
        "#### **Basic exploration of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvElHS-s5-37"
      },
      "outputs": [],
      "source": [
        "# Let's load the Breast Cancer Dataset from Scikit-Learn\n",
        "cancer_dataset = load_breast_cancer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AheW-MYn5-38"
      },
      "outputs": [],
      "source": [
        "# Keys in dataset\n",
        "cancer_dataset.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpWd2uH65-39"
      },
      "outputs": [],
      "source": [
        "# Malignant or benign value\n",
        "cancer_dataset['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX4HOCWR5-3-"
      },
      "outputs": [],
      "source": [
        "# Target value name malignant or benign tumor\n",
        "cancer_dataset['target_names']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU5TEtjS5-3_"
      },
      "outputs": [],
      "source": [
        "# Description of data\n",
        "print(cancer_dataset['DESCR'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt42UiBl5-3_"
      },
      "outputs": [],
      "source": [
        "# Name of features\n",
        "print(cancer_dataset['feature_names'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRtk7jgc5-4A"
      },
      "outputs": [],
      "source": [
        "# Create datafrmae\n",
        "cancer_df = pd.DataFrame(np.c_[cancer_dataset['data'],cancer_dataset['target']],\n",
        "             columns = np.append(cancer_dataset['feature_names'], ['target']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTvdPhpM5-4A"
      },
      "outputs": [],
      "source": [
        "# Head of cancer DataFrame\n",
        "cancer_df.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3eBHRdB5-4B"
      },
      "outputs": [],
      "source": [
        "# Tail of cancer DataFrame\n",
        "cancer_df.tail(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6gKBUJz5-4B"
      },
      "outputs": [],
      "source": [
        "# Information of cancer Dataframe\n",
        "cancer_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMiws6Ts5-4B"
      },
      "outputs": [],
      "source": [
        "# Numerical distribution of data\n",
        "cancer_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlY2_hX15-4C"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqw1Irnk5-4C"
      },
      "source": [
        "### **Helper Function**\n",
        "\n",
        "Evaluate Classifier - borrowed from the Notebook 00 Logistic Regression and Classification and Resampling methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH9YPYkK5-4C"
      },
      "outputs": [],
      "source": [
        "def evaluate_classifier(y_true, y_pred):\n",
        "    \"\"\"Print evaluation metrics for a classifier.\"\"\"\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztN8QUOq5-4I"
      },
      "source": [
        "### **Part 1: Model Selection and Regularization I**\n",
        "\n",
        "In this part we explore:\n",
        "\n",
        "- **Subset Selection:** using Recursive Feature Elimination (RFE)\n",
        "- **Ridge Regression (L2 Regularization) and Lasso Regression (L1 Regularization)**\n",
        "\n",
        "These methods help us control overfitting by penalizing large weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC8zzaxf5-4I"
      },
      "outputs": [],
      "source": [
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print(\"Full dataset shape:\", X.shape)\n",
        "\n",
        "# Subset Selection using Recursive Feature Elimination (RFE)\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "rfe = RFE(estimator=lda, n_features_to_select=5)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "print(\"Selected features (indices):\", np.where(rfe.support_)[0])\n",
        "lda.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = lda.predict(X_test_rfe)\n",
        "print(\"\\nSubset Selection (RFE) Evaluation:\")\n",
        "evaluate_classifier(y_test, y_pred_rfe)\n",
        "\n",
        "# Ridge Regression (L2 Regularization)\n",
        "class RidgeRegression(nn.Module):\n",
        "    def __init__(self, input_dim, lambda_reg=0.1):\n",
        "        super(RidgeRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "        self.lambda_reg = lambda_reg\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "    def ridge_penalty(self):\n",
        "        # L2 penalty\n",
        "        return self.lambda_reg * sum(torch.sum(param ** 2) for param in self.parameters())\n",
        "\n",
        "# Standardize features for Ridge regression\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1))\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "\n",
        "lambda_values = [0.0, 0.01, 0.1, 1.0]\n",
        "ridge_accuracies = []\n",
        "\n",
        "for l in lambda_values:\n",
        "    model_ridge = RidgeRegression(X_train_scaled.shape[1], lambda_reg=l)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model_ridge.parameters(), lr=0.01)\n",
        "    epochs = 1000\n",
        "    for epoch in range(epochs):\n",
        "        outputs = model_ridge(X_train_tensor)\n",
        "        loss = criterion(outputs, y_train_tensor) + model_ridge.ridge_penalty()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    model_ridge.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_probs = model_ridge(X_test_tensor)\n",
        "        y_pred = (y_pred_probs > 0.5).float().numpy().flatten()\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        ridge_accuracies.append(acc)\n",
        "    print(f\"Ridge: Lambda = {l}, Test Accuracy = {acc:.4f}\")\n",
        "\n",
        "# Plot Ridge results using Plotly\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=lambda_values, y=ridge_accuracies, mode='lines+markers'))\n",
        "fig.update_layout(\n",
        "    title=\"Effect of Ridge Regularization\",\n",
        "    xaxis=dict(title=\"Regularization Strength (λ)\", type=\"log\"),\n",
        "    yaxis_title=\"Accuracy\"\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# Lasso Regression (L1 Regularization)\n",
        "class LassoRegression(nn.Module):\n",
        "    def __init__(self, input_dim, lambda_reg=0.1):\n",
        "        super(LassoRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "        self.lambda_reg = lambda_reg\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "    def lasso_penalty(self):\n",
        "        # L1 penalty\n",
        "        return self.lambda_reg * sum(torch.sum(torch.abs(param)) for param in self.parameters())\n",
        "\n",
        "lambda_values = [0.0, 0.01, 0.1, 1.0]\n",
        "lasso_accuracies = []\n",
        "nonzero_coeffs = []\n",
        "\n",
        "for l in lambda_values:\n",
        "    model_lasso = LassoRegression(X_train_scaled.shape[1], lambda_reg=l)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model_lasso.parameters(), lr=0.01)\n",
        "    epochs = 1000\n",
        "    for epoch in range(epochs):\n",
        "        outputs = model_lasso(X_train_tensor)\n",
        "        loss = criterion(outputs, y_train_tensor) + model_lasso.lasso_penalty()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    model_lasso.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_probs = model_lasso(X_test_tensor)\n",
        "        y_pred = (y_pred_probs > 0.5).float().numpy().flatten()\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        lasso_accuracies.append(acc)\n",
        "        # Count non-zero weights\n",
        "        weight = model_lasso.linear.weight.data.numpy().flatten()\n",
        "        nonzeros = np.sum(np.abs(weight) > 0.01)\n",
        "        nonzero_coeffs.append(nonzeros)\n",
        "    print(f\"Lasso: Lambda = {l}, Test Accuracy = {acc:.4f}, Non-zero Coeffs = {nonzeros}/{len(weight)}\")\n",
        "\n",
        "# Plot Lasso results using Plotly (Accuracy and sparsity)\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=lambda_values, y=lasso_accuracies, mode='lines+markers', name=\"Accuracy\"))\n",
        "fig.update_layout(\n",
        "    title=\"Effect of Lasso Regularization on Accuracy\",\n",
        "    xaxis=dict(title=\"Regularization Strength (λ)\", type=\"log\"),\n",
        "    yaxis_title=\"Accuracy\"\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=lambda_values, y=nonzero_coeffs, mode='lines+markers', name=\"Non-zero Coefficients\"))\n",
        "fig.update_layout(\n",
        "    title=\"Lasso Regularization: Sparsity\",\n",
        "    xaxis=dict(title=\"Regularization Strength (λ)\", type=\"log\"),\n",
        "    yaxis_title=\"Number of Non-zero Coefficients\"\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7m2X_Bj5-4J"
      },
      "source": [
        "### **Part 2: Model Selection and Regularization II (PyTorch)**\n",
        "\n",
        "This section integrates model selection using PyTorch implementations along with hyperparameter tuning\n",
        "of different methods including:\n",
        "\n",
        "- **Logistic Regression with Regularization**\n",
        "- **k-Nearest Neighbors (kNN)**\n",
        "- **Random Forest**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAT5Qewz5-4K"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the breast cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1))\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1))\n",
        "\n",
        "# Logistic Regression with Regularization in PyTorch\n",
        "class LogisticRegressionWithReg(nn.Module):\n",
        "    def __init__(self, input_dim, l1_lambda=0.0, l2_lambda=0.0):\n",
        "        super(LogisticRegressionWithReg, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "        self.l1_lambda = l1_lambda\n",
        "        self.l2_lambda = l2_lambda\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "    def regularization_loss(self):\n",
        "        l1 = self.l1_lambda * sum(torch.sum(torch.abs(param)) for param in self.parameters())\n",
        "        l2 = self.l2_lambda * sum(torch.sum(param ** 2) for param in self.parameters())\n",
        "        return l1 + l2\n",
        "\n",
        "# Cross-validation function for PyTorch models\n",
        "def cross_validate_model(X, y, model_class, model_params, cv=5, epochs=500, batch_size=32, lr=0.01):\n",
        "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "        X_train_tensor = torch.FloatTensor(X_train_fold)\n",
        "        y_train_tensor = torch.FloatTensor(y_train_fold).reshape(-1, 1)\n",
        "        X_val_tensor = torch.FloatTensor(X_val_fold)\n",
        "\n",
        "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "        loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        model = model_class(**model_params)\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for inputs, labels in loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                if hasattr(model, 'regularization_loss'):\n",
        "                    loss += model.regularization_loss()\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(X_val_tensor)\n",
        "            y_pred = (outputs > 0.5).float().numpy().flatten()\n",
        "            acc = accuracy_score(y_val_fold, y_pred)\n",
        "            scores.append(acc)\n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "model_params = {'input_dim': input_dim, 'l1_lambda': 0.01, 'l2_lambda': 0.01}\n",
        "mean_acc, std_acc = cross_validate_model(X_train_scaled, y_train, LogisticRegressionWithReg, model_params, cv=5, epochs=500)\n",
        "print(f\"Logistic Regression with Regularization: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
        "\n",
        "# Train final logistic regression model with regularization on full training set\n",
        "best_model = LogisticRegressionWithReg(input_dim, l1_lambda=0.01, l2_lambda=0.01)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(best_model.parameters(), lr=0.01)\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "for epoch in range(500):\n",
        "    for inputs, labels in loader:\n",
        "        outputs = best_model(inputs)\n",
        "        loss = criterion(outputs, labels) + best_model.regularization_loss()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "best_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = best_model(X_test_tensor)\n",
        "    y_pred_logreg = (outputs > 0.5).float().numpy().flatten()\n",
        "    logreg_test_acc = accuracy_score(y_test, y_pred_logreg)\n",
        "print(f\"Final Logistic Regression with Reg - Test Accuracy: {logreg_test_acc:.4f}\")\n",
        "\n",
        "# k-Nearest Neighbors (kNN)\n",
        "param_grid = {'n_neighbors': [3, 5, 7, 9]}\n",
        "grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "grid_search_knn.fit(X_train_scaled, y_train)\n",
        "print(f\"Best k for kNN: {grid_search_knn.best_params_['n_neighbors']}, CV Accuracy: {grid_search_knn.best_score_:.4f}\")\n",
        "best_knn = KNeighborsClassifier(n_neighbors=grid_search_knn.best_params_['n_neighbors'])\n",
        "best_knn.fit(X_train_scaled, y_train)\n",
        "y_pred_knn = best_knn.predict(X_test_scaled)\n",
        "knn_test_acc = accuracy_score(y_test, y_pred_knn)\n",
        "print(f\"kNN Test Accuracy: {knn_test_acc:.4f}\")\n",
        "\n",
        "# Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30]\n",
        "}\n",
        "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search_rf.fit(X_train_scaled, y_train)\n",
        "print(f\"Best Params for Random Forest: {grid_search_rf.best_params_}, CV Accuracy: {grid_search_rf.best_score_:.4f}\")\n",
        "best_rf = RandomForestClassifier(**grid_search_rf.best_params_, random_state=42)\n",
        "best_rf.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = best_rf.predict(X_test_scaled)\n",
        "rf_test_acc = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Test Accuracy: {rf_test_acc:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Model Comparison on Test Set:\")\n",
        "print(f\"Logistic Regression with Reg: {logreg_test_acc:.4f}\")\n",
        "print(f\"kNN: {knn_test_acc:.4f}\")\n",
        "print(f\"Random Forest: {rf_test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSJzApOb5-4K"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riVlGq3O5-4K"
      },
      "source": [
        "## **REFERENCES**\n",
        "\n",
        "[1] Wolberg, W., Mangasarian, O., Street, N., & Street, W. (1993). Breast Cancer Wisconsin (Diagnostic) [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5DW2B.\n",
        "\n",
        "[2] Jurafsky and Martin. (2025). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models, 3rd edition. Ch. 5. Logistic Regression. Online manuscript released January 12, 2025. https://web.stanford.edu/~jurafsky/slp3."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}