{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_HSgEpR5-3x"
      },
      "source": [
        "### **State University of Campinas - UNICAMP** </br>\n",
        "**Course**: MC886A </br>\n",
        "**Professor**: Marcelo da Silva Reis </br>\n",
        "**TA (PED)**: Marcos Vinicius Souza Freire\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFUBcOBT5-32"
      },
      "source": [
        "### **Hands-On: Logistic Regression, Classification Methods, and Resampling Methods**\n",
        "##### Notebook: 00 Logistic Regression and Classification and Resampling methods\n",
        "\n",
        "> Dataset from Scikit Learn - [load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html), based on [Breast Cancer Wisconsin (Diagnostic)](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)(1993)[1]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYQbko5s5-33"
      },
      "source": [
        "**This notebook covers the following topics:**\n",
        "\n",
        "- **Logistic Regression:** binary, multiple, and multinomial extensions.\n",
        "- **Classification Methods:** Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA) and Naive Bayes.\n",
        "- **Resampling Methods:** Leave-One-Out (LOOCV), k-Fold Cross-Validation and Bootstrap.\n",
        "\n",
        "Throughout the notebook we illustrate the methods using formulas, interactive Plotly graphs for the decision boundaries, and well-structured code cells.\n",
        "\n",
        "---\n",
        "\n",
        "### **Notation and Formulas**\n",
        "\n",
        "\n",
        "### **1. Binary Logistic Regression**  \n",
        "Used for **two-class classification** (e.g., yes/no, 0/1).\n",
        "\n",
        "#### **Formula**:  \n",
        "The probability $ p $ that an instance belongs to class $ y=1 $ is modeled using the **sigmoid function**:  \n",
        "$\n",
        "p(y=1 \\mid \\mathbf{x}) = \\frac{1}{1 + e^{-z}}, \\quad \\text{where } z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
        "$  \n",
        "- $ \\mathbf{x} = [x_1, x_2, \\dots, x_n] $: Input features.  \n",
        "- $ \\beta_0, \\beta_1, \\dots, \\beta_n $: Model coefficients.  \n",
        "- **Decision**: Classify as $ y=1 $ if $ p \\geq 0.5 $, else $ y=0 $.  \n",
        "\n",
        "#### **Logit (Log-Odds)**:  \n",
        "$\n",
        "\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n",
        "$  \n",
        "\n",
        "\n",
        "This linear equation represents the relationship between features and the log-odds of the positive class.\n",
        "\n",
        "</br>\n",
        "\n",
        "#### **Loss Function (Cross-Entropy Loss)**:  \n",
        "$\n",
        "\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i) \\right]\n",
        "$  \n",
        "\n",
        "</br>\n",
        "\n",
        "Minimizing this loss adjusts the coefficients $ \\beta $ via gradient descent or MLE.\n",
        "\n",
        "---\n",
        "</br>\n",
        "\n",
        "### **2. Multinomial Logistic Regression**  \n",
        "Used for **multi-class classification** (e.g., classes A/B/C/D).\n",
        "\n",
        "#### **Main Formula**:  \n",
        "The probability $ p(y=k \\mid \\mathbf{x}) $ for class $ k $ is modeled using the **softmax function**:  \n",
        "$\n",
        "p(y=k \\mid \\mathbf{x}) = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}, \\quad \\text{where } z_k = \\beta_{k0} + \\beta_{k1} x_1 + \\dots + \\beta_{kn} x_n\n",
        "$  \n",
        "- $ K $: Total number of classes.  \n",
        "- $ z_k $: Linear combination for class $ k $.  \n",
        "- **Decision**: Classify as the class with the highest probability $ p(y=k \\mid \\mathbf{x}) $.  \n",
        "\n",
        "#### **Key Notes**:  \n",
        "- One class (e.g., $ K $) is typically treated as the **reference category**, and its coefficients are set to zero (e.g., $ z_K = 0 $).  \n",
        "- The model estimates $ K-1 $ sets of coefficients.  \n",
        "\n",
        "#### **Loss Function (Generalized Cross-Entropy)**:  \n",
        "$\n",
        "\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^K y_{ik} \\ln(p_{ik})\n",
        "$  \n",
        "- $ y_{ik} = 1 $ if observation $ i $ is in class $ k $, else 0.  \n",
        "- $ p_{ik} $: Predicted probability that observation $ i $ belongs to class $ k $.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **Differences**  \n",
        "| **Aspect**              | **Binary**                          | **Multinomial**                     |  \n",
        "|--------------------------|-------------------------------------|-------------------------------------|  \n",
        "| **Classes**              | 2 classes (0/1)                    | $ K \\geq 2 $ classes              |  \n",
        "| **Function**             | Sigmoid                            | Softmax                             |  \n",
        "| **Coefficients**         | One set ($ \\beta_0, \\beta_1, \\dots $) | $ K-1 $ sets (one per class) |  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example Applications**  \n",
        "1. **Binary**:  \n",
        "   - Predict if an email is spam ($ p \\geq 0.5 $) or not.  \n",
        "   - Compute $ z = 2.5 + 0.8x_1 - 1.2x_2 $, then $ p = \\frac{1}{1 + e^{-z}} $.  \n",
        "\n",
        "2. **Multinomial**:  \n",
        "   - Classify an image into \"cat,\" \"dog,\" or \"bird.\"  \n",
        "   - For features $ \\mathbf{x} $, compute probabilities:  \n",
        "     $\n",
        "     p(\\text{cat}) = \\frac{e^{z_{\\text{cat}}}}{e^{z_{\\text{cat}}} + e^{z_{\\text{dog}}} + e^{z_{\\text{bird}}}}\n",
        "     $  \n",
        "     (Similarly for other classes.)\n",
        "\n",
        "\n",
        "Based on the Jurafsky & Martin (2025) lectures [2]\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPViT5TV5-34"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Replace Matplotlib with Plotly for interactive plotting\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.datasets import make_classification, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdR1ViJB5-37"
      },
      "source": [
        "#### **Basic exploration of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvElHS-s5-37"
      },
      "outputs": [],
      "source": [
        "# Let's load the Breast Cancer Dataset from Scikit-Learn\n",
        "cancer_dataset = load_breast_cancer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AheW-MYn5-38"
      },
      "outputs": [],
      "source": [
        "# Keys in dataset\n",
        "cancer_dataset.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpWd2uH65-39"
      },
      "outputs": [],
      "source": [
        "# Malignant or benign value\n",
        "cancer_dataset['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX4HOCWR5-3-"
      },
      "outputs": [],
      "source": [
        "# Target value name malignant or benign tumor\n",
        "cancer_dataset['target_names']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU5TEtjS5-3_"
      },
      "outputs": [],
      "source": [
        "# Description of data\n",
        "print(cancer_dataset['DESCR'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt42UiBl5-3_"
      },
      "outputs": [],
      "source": [
        "# Name of features\n",
        "print(cancer_dataset['feature_names'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRtk7jgc5-4A"
      },
      "outputs": [],
      "source": [
        "# Create datafrmae\n",
        "cancer_df = pd.DataFrame(np.c_[cancer_dataset['data'],cancer_dataset['target']],\n",
        "             columns = np.append(cancer_dataset['feature_names'], ['target']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTvdPhpM5-4A"
      },
      "outputs": [],
      "source": [
        "# Head of cancer DataFrame\n",
        "cancer_df.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3eBHRdB5-4B"
      },
      "outputs": [],
      "source": [
        "# Tail of cancer DataFrame\n",
        "cancer_df.tail(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6gKBUJz5-4B"
      },
      "outputs": [],
      "source": [
        "# Information of cancer Dataframe\n",
        "cancer_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMiws6Ts5-4B"
      },
      "outputs": [],
      "source": [
        "# Numerical distribution of data\n",
        "cancer_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlY2_hX15-4C"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqw1Irnk5-4C"
      },
      "source": [
        "### **Helper Functions**\n",
        "\n",
        "In this section, we define helper functions to evaluate classifiers and to plot decision boundaries.\n",
        "\n",
        "**Decision Boundary Plotting with Plotly:**\n",
        "\n",
        "We create a function (`plot_decision_boundary_plotly`) that plots the decision boundary using Plotly. For a given model, we generate a grid over the feature space, predict the classes for each grid point, and then plot a contour along with the data points.\n",
        "\n",
        "Also, we implement the `evaluate_classifier`, which prints:\n",
        "\n",
        "1. **Accuracy**\n",
        "   - **What It Measures**: Accuracy is the proportion of predictions the classifier got right out of all predictions made, i.e., the proportion of correctly classified instances.\n",
        "   - **Formula**:\n",
        "   $\n",
        "   \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
        "   $\n",
        "   - **Output**: A single number between 0 and 1 (e.g., `0.85` means 85% of predictions were correct).\n",
        "   - **Why It Matters**: It gives a quick representation of overall performance but can be misleading if your dataset has imbalanced classes (e.g., 90% of one class and 10% of another).\n",
        "\n",
        "2. **Confusion Matrix**\n",
        "   - **What It Shows**: This is a table that counts how many times the classifier predicted each class correctly or incorrectly compared to the true labels.\n",
        "\n",
        "   - **Structure** (for binary classification):\n",
        "\n",
        "|                | Predicted Positive | Predicted Negative |\n",
        "|----------------|--------------------|--------------------|\n",
        "| **Actual Positive** | True Positives (TP) | False Negatives (FN) |\n",
        "| **Actual Negative** | False Positives (FP) | True Negatives (TN)  |\n",
        "\n",
        "   - **Example Output**:\n",
        "     ```\n",
        "     [[50  5]\n",
        "      [10 35]]\n",
        "     ```\n",
        "     Here, 50 true negatives, 5 false positives, 10 false negatives, and 35 true positives.\n",
        "   - **Why It Matters**: It reveals the specific types of errors (e.g., mistaking positives for negatives), which is crucial for understanding model behavior beyond just accuracy.\n",
        "\n",
        "3. **Classification Report**\n",
        "   - **What It Provides**: A detailed summary of performance for each class, including:\n",
        "     - **Precision**: How many of the predicted positives are actually positive.\n",
        "       $\n",
        "       \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
        "       $\n",
        "     - **Recall**: How many of the actual positives were correctly predicted.\n",
        "       $\n",
        "       \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
        "       $\n",
        "     - **F1-Score**: A balanced measure combining precision and recall.\n",
        "       $\n",
        "       \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "       $\n",
        "     - **Support**: The number of true instances (number of occurrences) of each class in the dataset.\n",
        "   - **Example Output**:\n",
        "     ```\n",
        "                  precision    recall  f1-score   support\n",
        "            0       0.83      0.91      0.87        55\n",
        "            1       0.88      0.78      0.82        45\n",
        "     accuracy                            0.85       100\n",
        "     ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH9YPYkK5-4C"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary_plotly(model, X, y, title=\"Decision Boundary\"):\n",
        "    \"\"\"\n",
        "    Plot the decision boundary using Plotly. Works for both PyTorch and sklearn models.\n",
        "    \"\"\"\n",
        "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
        "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "    # Prediction logic for PyTorch and scikit-learn models\n",
        "    if hasattr(model, 'forward'):\n",
        "        # For PyTorch models: using forward pass\n",
        "        with torch.no_grad():\n",
        "            grid_tensor = torch.FloatTensor(grid)\n",
        "            outputs = model(grid_tensor)\n",
        "            # Multi-class case\n",
        "            if outputs.ndim > 1 and outputs.shape[1] > 1:\n",
        "                Z = np.argmax(outputs.numpy(), axis=1)\n",
        "            else:\n",
        "                Z = (outputs.numpy() > 0.5).astype(int).reshape(-1)\n",
        "    else:\n",
        "        # For scikit-learn models\n",
        "        Z = model.predict(grid)\n",
        "\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Create contour plot with Plotly\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(\n",
        "        go.Contour(\n",
        "            x=np.linspace(x_min, x_max, 200),\n",
        "            y=np.linspace(y_min, y_max, 200),\n",
        "            z=Z,\n",
        "            colorscale='Viridis',\n",
        "            opacity=0.3,\n",
        "            showscale=False\n",
        "        )\n",
        "    )\n",
        "    # Scatter plot for data points\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=X[:, 0],\n",
        "            y=X[:, 1],\n",
        "            mode=\"markers\",\n",
        "            marker=dict(\n",
        "                color=y,\n",
        "                colorscale='Viridis',\n",
        "                line=dict(width=1, color='black')\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title='Feature 1',\n",
        "        yaxis_title='Feature 2'\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "def evaluate_classifier(y_true, y_pred):\n",
        "    \"\"\"Print evaluation metrics for a classifier.\"\"\"\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "TGufzReyEYY5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyqEk18q5-4C"
      },
      "source": [
        "### **Part 1: Logistic Regression**\n",
        "\n",
        "In this section, we will cover:\n",
        "\n",
        "- **Binary Logistic Regression:** using a simple PyTorch model with a sigmoid function.\n",
        "- **Multiple Logistic Regression:** applying the method on a breast cancer dataset.\n",
        "- **Multinomial Logistic Regression:** extending logistic regression to handle multi-class cases using the softmax function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htg3DQNS5-4D"
      },
      "source": [
        "#### **1.1 Binary Logistic Regression with PyTorch**\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "- **Sigmoid Function:**  \n",
        "   $\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $\n",
        "\n",
        "- **Loss Function:** Binary Cross-Entropy loss is used.\n",
        "\n",
        "We generate a synthetic dataset (with two features) for binary classification, standardize the features, and define a simple PyTorch model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc50d3Rx5-4D"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic data for binary classification\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
        "                           n_informative=2, random_state=42, n_clusters_per_class=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define PyTorch model for logistic regression\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1))\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1))\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "model_binary = LogisticRegressionModel(input_dim)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model_binary.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        outputs = model_binary(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate model on test set\n",
        "model_binary.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_probs = model_binary(X_test_tensor)\n",
        "    y_pred = (y_pred_probs > 0.5).float().numpy().flatten()\n",
        "\n",
        "print(\"\\nBinary Logistic Regression Evaluation:\")\n",
        "evaluate_classifier(y_test, y_pred)\n",
        "\n",
        "# Plot decision boundary using Plotly\n",
        "plot_decision_boundary_plotly(model_binary, X_train_scaled, y_train, title=\"Binary Logistic Regression (Training Data)\")\n",
        "plot_decision_boundary_plotly(model_binary, X_test_scaled, y_test, title=\"Binary Logistic Regression (Test Data)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro6eh2ii5-4D"
      },
      "source": [
        "#### **1.2 Multiple Logistic Regression**\n",
        "\n",
        "Here we use the Breast Cancer dataset (which has multiple features) to demonstrate logistic regression on a real-world, higher-dimensional dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKaZ9roy5-4E"
      },
      "outputs": [],
      "source": [
        "data = load_breast_cancer()\n",
        "X_multi = data.data\n",
        "y_multi = data.target\n",
        "print(f\"Dataset: Breast Cancer Dataset with {X_multi.shape[1]} features\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_multi, y_multi, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1))\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "\n",
        "# Initialize and train model\n",
        "model_multi = LogisticRegressionModel(X_train_scaled.shape[1])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model_multi.parameters(), lr=0.01)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        outputs = model_multi(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the multiple logistic regression model\n",
        "model_multi.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_probs = model_multi(X_test_tensor)\n",
        "    y_pred = (y_pred_probs > 0.5).float().numpy().flatten()\n",
        "\n",
        "print(\"\\nMultiple Logistic Regression Evaluation:\")\n",
        "evaluate_classifier(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnFj20pO5-4E"
      },
      "source": [
        "#### **1.3 Multinomial Logistic Regression**\n",
        "\n",
        "For multiclass classification we define a model using the softmax output. The softmax function is given by:\n",
        "\n",
        "$\n",
        "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
        "$\n",
        "\n",
        "We generate a synthetic multi-class dataset and then train a PyTorch model using the cross-entropy loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJKOroHY5-4E"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic data for multi-class classification\n",
        "X_multiclass, y_multiclass = make_classification(n_samples=500, n_features=2, n_informative=2,\n",
        "                                                 n_redundant=0, n_classes=3, n_clusters_per_class=1,\n",
        "                                                 random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_multiclass, y_multiclass, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define a multinomial logistic regression model using softmax\n",
        "class MultinomialLogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(MultinomialLogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.softmax(self.linear(x), dim=1)\n",
        "\n",
        "# Convert data to tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "\n",
        "# Initialize model, loss (cross-entropy) and optimizer\n",
        "model_multi_class = MultinomialLogisticRegression(X_train_scaled.shape[1], 3)\n",
        "criterion_multi = nn.CrossEntropyLoss()\n",
        "optimizer_multi = optim.SGD(model_multi_class.parameters(), lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    outputs = model_multi_class(X_train_tensor)\n",
        "    loss = criterion_multi(outputs, y_train_tensor)\n",
        "\n",
        "    optimizer_multi.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_multi.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the multinomial model\n",
        "model_multi_class.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model_multi_class(X_test_tensor)\n",
        "    _, y_pred_tensor = torch.max(outputs, 1)\n",
        "    y_pred = y_pred_tensor.numpy()\n",
        "\n",
        "print(\"\\nMultinomial Logistic Regression Evaluation:\")\n",
        "evaluate_classifier(y_test, y_pred)\n",
        "\n",
        "# Plot decision boundary using Plotly\n",
        "plot_decision_boundary_plotly(model_multi_class, X_test_scaled, y_test, title=\"Multinomial Logistic Regression\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ2bu3zQ5-4E"
      },
      "source": [
        "### **Part 2: LDA and Other Classification Methods**\n",
        "\n",
        "In this section, we apply classic classification techniques:\n",
        "\n",
        "- **Linear Discriminant Analysis (LDA)**\n",
        "- **Quadratic Discriminant Analysis (QDA)**\n",
        "- **Naive Bayes**\n",
        "\n",
        "These methods are demonstrated on a synthetic two-feature dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7VI13y45-4F"
      },
      "outputs": [],
      "source": [
        "# Generate and standardize data for LDA/QDA/Naive Bayes\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
        "                           n_informative=2, random_state=42, n_clusters_per_class=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Linear Discriminant Analysis (LDA)\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda.fit(X_train_scaled, y_train)\n",
        "y_pred_lda = lda.predict(X_test_scaled)\n",
        "print(\"LDA Evaluation:\")\n",
        "evaluate_classifier(y_test, y_pred_lda)\n",
        "plot_decision_boundary_plotly(lda, X_test_scaled, y_test, title=\"LDA Decision Boundary\")\n",
        "\n",
        "# Quadratic Discriminant Analysis (QDA)\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "qda.fit(X_train_scaled, y_train)\n",
        "y_pred_qda = qda.predict(X_test_scaled)\n",
        "print(\"\\nQDA Evaluation:\")\n",
        "evaluate_classifier(y_test, y_pred_qda)\n",
        "plot_decision_boundary_plotly(qda, X_test_scaled, y_test, title=\"QDA Decision Boundary\")\n",
        "\n",
        "# Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train_scaled, y_train)\n",
        "y_pred_nb = nb.predict(X_test_scaled)\n",
        "print(\"\\nNaive Bayes Evaluation:\")\n",
        "evaluate_classifier(y_test, y_pred_nb)\n",
        "plot_decision_boundary_plotly(nb, X_test_scaled, y_test, title=\"Naive Bayes Decision Boundary\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYVCMU6o5-4F"
      },
      "outputs": [],
      "source": [
        "# Generate and standardize data for LDA/QDA/Naive Bayes\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
        "                           n_informative=2, random_state=42, n_clusters_per_class=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# Train LDA, QDA, and Naive Bayes models\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda.fit(X_train_scaled, y_train)\n",
        "\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "qda.fit(X_train_scaled, y_train)\n",
        "\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Create a common meshgrid over the feature space\n",
        "x_min, x_max = X_test_scaled[:, 0].min() - 0.1, X_test_scaled[:, 0].max() + 0.1\n",
        "y_min, y_max = X_test_scaled[:, 1].min() - 0.1, X_test_scaled[:, 1].max() + 0.1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                     np.linspace(y_min, y_max, 200))\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "# For each classifier, get the probability for class 1 over the grid\n",
        "# Note: For binary classification, the decision boundary is where P(class=1)=0.5\n",
        "Z_lda = lda.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
        "Z_qda = qda.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
        "Z_nb  = nb.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
        "\n",
        "# Create combined Plotly figure with contours from each classifier\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# LDA decision boundary (blue)\n",
        "fig.add_trace(go.Contour(\n",
        "    x=np.linspace(x_min, x_max, 200),\n",
        "    y=np.linspace(y_min, y_max, 200),\n",
        "    z=Z_lda,\n",
        "    contours=dict(\n",
        "        start=0.5,\n",
        "        end=0.5,\n",
        "        size=0.01,\n",
        "        coloring='lines'\n",
        "    ),\n",
        "    line=dict(color='blue', width=2),\n",
        "    # Force the colorscale to be solid blue:\n",
        "    colorscale=[[0, 'blue'], [1, 'blue']],\n",
        "    showscale=False,\n",
        "    name='LDA'\n",
        "))\n",
        "\n",
        "# QDA decision boundary (red)\n",
        "fig.add_trace(go.Contour(\n",
        "    x=np.linspace(x_min, x_max, 200),\n",
        "    y=np.linspace(y_min, y_max, 200),\n",
        "    z=Z_qda,\n",
        "    contours=dict(\n",
        "        start=0.5,\n",
        "        end=0.5,\n",
        "        size=0.01,\n",
        "        coloring='lines'\n",
        "    ),\n",
        "    line=dict(color='red', width=2),\n",
        "    colorscale=[[0, 'red'], [1, 'red']],\n",
        "    showscale=False,\n",
        "    name='QDA'\n",
        "))\n",
        "\n",
        "# Naive Bayes decision boundary (green)\n",
        "fig.add_trace(go.Contour(\n",
        "    x=np.linspace(x_min, x_max, 200),\n",
        "    y=np.linspace(y_min, y_max, 200),\n",
        "    z=Z_nb,\n",
        "    contours=dict(\n",
        "        start=0.5,\n",
        "        end=0.5,\n",
        "        size=0.01,\n",
        "        coloring='lines'\n",
        "    ),\n",
        "    line=dict(color='green', width=2),\n",
        "    colorscale=[[0, 'green'], [1, 'green']],\n",
        "    showscale=False,\n",
        "    name='Naive Bayes'\n",
        "))\n",
        "\n",
        "# Add scatter plot for the test data\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=X_test_scaled[:, 0],\n",
        "    y=X_test_scaled[:, 1],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        color=y_test,\n",
        "        colorscale='Viridis',\n",
        "        line=dict(width=1, color='black')\n",
        "    ),\n",
        "    name='Test Data'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Combined Decision Boundaries: LDA (blue), QDA (red), Naive Bayes (green)',\n",
        "    xaxis_title='Feature 1',\n",
        "    yaxis_title='Feature 2'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLmdCXAI5-4G"
      },
      "source": [
        "### **Part 3: Resampling Methods**\n",
        "\n",
        "Here we illustrate the following resampling techniques:\n",
        "\n",
        "- **Leave-One-Out Cross-Validation (LOOCV)**\n",
        "- **K-Fold Cross-Validation**\n",
        "- **Bootstrap**\n",
        "\n",
        "These techniques are useful for assessing model generalization and understanding the $bias-variance \\space \\space trade-off$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a--DG4IU5-4G"
      },
      "outputs": [],
      "source": [
        "# Using the Breast Cancer dataset to demonstrate resampling methods\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "\n",
        "# LOOCV\n",
        "loocv = LeaveOneOut()\n",
        "model = LinearDiscriminantAnalysis()\n",
        "loocv_scores = cross_val_score(model, X, y, cv=loocv, scoring='accuracy')\n",
        "print(f\"\\nLOOCV - Mean Accuracy: {loocv_scores.mean():.4f}, Std: {loocv_scores.std():.4f}\")\n",
        "\n",
        "# K-Fold Cross-Validation for different k values\n",
        "for k in [5, 10]:\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
        "    print(f\"{k}-Fold CV - Mean Accuracy: {cv_scores.mean():.4f}, Std: {cv_scores.std():.4f}\")\n",
        "\n",
        "# Bias-Variance Trade-off demonstration\n",
        "k_range = [2, 5, 10, 20, len(X)]  # last one is LOOCV\n",
        "mean_scores = []\n",
        "std_scores = []\n",
        "for k in k_range:\n",
        "    if k == len(X):\n",
        "        cv = LeaveOneOut()\n",
        "        label = \"LOOCV\"\n",
        "    else:\n",
        "        cv = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "        label = f\"{k}-fold\"\n",
        "    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "    mean_scores.append(cv_scores.mean())\n",
        "    std_scores.append(cv_scores.std())\n",
        "    print(f\"{label} - Mean Accuracy: {cv_scores.mean():.4f}, Std: {cv_scores.std():.4f}\")\n",
        "\n",
        "# Plot bias-variance trade-off with Plotly\n",
        "fig = go.Figure()\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=[str(k) if k != len(X) else \"LOOCV\" for k in k_range],\n",
        "        y=mean_scores,\n",
        "        error_y=dict(type='data', array=std_scores, visible=True),\n",
        "        mode='lines+markers'\n",
        "    )\n",
        ")\n",
        "fig.update_layout(\n",
        "    title='Bias-Variance Trade-off in K-Fold Cross-Validation',\n",
        "    xaxis_title='Number of Folds (k)',\n",
        "    yaxis_title='Accuracy'\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# Bootstrap\n",
        "def bootstrap(X, y, model, n_bootstraps=1000):\n",
        "    n_samples = len(X)\n",
        "    scores = []\n",
        "    for _ in range(n_bootstraps):\n",
        "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        X_boot, y_boot = X[indices], y[indices]\n",
        "        # Out-of-bag samples\n",
        "        oob_indices = list(set(range(n_samples)) - set(indices))\n",
        "        if not oob_indices:\n",
        "            continue  # skip if no out-of-bag samples (rare)\n",
        "        X_oob, y_oob = X[oob_indices], y[oob_indices]\n",
        "        model.fit(X_boot, y_boot)\n",
        "        scores.append(model.score(X_oob, y_oob))\n",
        "    return scores\n",
        "\n",
        "bootstrap_scores = bootstrap(X, y, LinearDiscriminantAnalysis(), n_bootstraps=100)\n",
        "print(f\"\\nBootstrap - Mean Accuracy: {np.mean(bootstrap_scores):.4f}, Std: {np.std(bootstrap_scores):.4f}\")\n",
        "\n",
        "# Plot bootstrap distribution using Plotly\n",
        "fig = px.histogram(bootstrap_scores, nbins=20, title='Bootstrap Distribution of LDA Accuracy')\n",
        "fig.add_vline(x=np.mean(bootstrap_scores), line_dash=\"dash\", line_color=\"red\",\n",
        "              annotation_text=f\"Mean={np.mean(bootstrap_scores):.4f}\")\n",
        "fig.update_layout(xaxis_title=\"Accuracy\", yaxis_title=\"Frequency\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riVlGq3O5-4K"
      },
      "source": [
        "## **REFERENCES**\n",
        "\n",
        "[1] Wolberg, W., Mangasarian, O., Street, N., & Street, W. (1993). Breast Cancer Wisconsin (Diagnostic) [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5DW2B.\n",
        "\n",
        "[2] Jurafsky and Martin. (2025). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models, 3rd edition. Ch. 5. Logistic Regression. Online manuscript released January 12, 2025. https://web.stanford.edu/~jurafsky/slp3."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}