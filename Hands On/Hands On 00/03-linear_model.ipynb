{"cells":[{"cell_type":"markdown","metadata":{"id":"io_JPlimJILM"},"source":["### **State University of Campinas - UNICAMP** </br>\n","**Course**: MC886A </br>\n","**Professor**: Marcelo da Silva Reis </br>\n","**TA (PED)**: Marcos Vinicius Souza Freire\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"u7dMxrGaJILR"},"source":["### **Hands-On: Introduction to Machine Learning and Tensors in PyTorch**\n","##### Notebook: 03 Linear Model\n","\n","> Based on Explore mtcars by Krasser (2024)[1] [https://cran.r-project.org/web/packages/explore/vignettes/explore-mtcars.html](https://cran.r-project.org/web/packages/explore/vignettes/explore-mtcars.html)\n","---"]},{"cell_type":"markdown","metadata":{"id":"xrTjoEP9JILR"},"source":["### **Table of Contents**\n","\n","1. [**Objectives**](#objectives) </br>\n","2. [**Exploratory Data Analysis (EDA)**](#2-exploratory-data-analysis-eda) </br>\n","3. [**Mathematical Theory of Linear Regression**](#3-mathematical-theory-of-linear-regression) </br>\n","4. [**Building the Linear Model in PyTorch**](#4-building-the-linear-model-in-pytorch) </br>\n","5. [**Extending to a Deep Model**](#5-extending-to-a-deep-model) </br>\n","6. [**Discussion**](#6-discussion) </br>\n","7. [**REFERENCES**](#references)\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"PIcheqtbJILS"},"source":["#### Objective\n","- **Understand the Data:** Explore the `mtcars` dataset to identify patterns.\n","- **Build a Model:** Implement linear regression in PyTorch to predict `mpg`.\n","- **Learn the Theory:** Connect code to the math behind linear regression.\n","- **Discuss Results:** Interpret model performance and weights."]},{"cell_type":"markdown","metadata":{"id":"YdcL4PWHJILS"},"source":["- **What is mtcars?**\n","  - We'll work with the `mtcars` dataset from 1974 Motor Trend magazine. It has 32 cars with 11 features like miles per gallon (`mpg`), weight (`wt`), horsepower (`hp`), cylinders (`cyl`), and gears (`gear`). Our task is to predict `mpg` using these features.\n","  - Show the first 5 rows (your output):\n","    ```\n","    First 5 rows of mtcars:\n","                        mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  carb\n","    Mazda RX4          21.0    6  160.0  110  3.90  2.620  16.46   0   1     4     4\n","    Mazda RX4 Wag      21.0    6  160.0  110  3.90  2.875  17.02   0   1     4     4\n","    Datsun 710         22.8    4  108.0   93  3.85  2.320  18.61   1   1     4     1\n","    Hornet 4 Drive     21.4    6  258.0  110  3.08  3.215  19.44   1   0     3     1\n","    Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3     2\n","    ```"]},{"cell_type":"markdown","metadata":{"id":"sLwNZihfJILT"},"source":["- **Why Linear Regression?**\n","  - Linear regression is a foundational ML technique. It assumes a linear relationship between inputs (e.g., `wt`, `hp`) and output (`mpg`). We’ll use PyTorch to build it and see how well it works.\n","\n","- **Plan:**\n","  - Explore data → Build a simple model → Extend to a deep model → Discuss results → Tie to math.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"JEcCLbFqJILU"},"source":["#### **2. Exploratory Data Analysis (EDA)**"]},{"cell_type":"markdown","metadata":{"id":"ht88qMcoJILU"},"source":["- **Code Demo:**"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"6f-VfqjBJZnO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q9UTmkM3JILV"},"outputs":[],"source":["import pandas as pd\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","# Load data\n","mtcars = pd.read_csv('/content/drive/MyDrive/TA-PED/CLASSROOM/hands-on_00/data/mtcars.csv', index_col=0)\n","\n","# Histogram of 'gear'\n","fig = px.histogram(mtcars, x='gear', title='Number of Cars by Gear')\n","fig.show()\n","\n","# Scatter plot: Weight vs MPG\n","fig = px.scatter(mtcars, x='wt', y='mpg', trendline='ols',\n","                title='Weight vs MPG', labels={'wt': 'Weight (1000 lbs)', 'mpg': 'Miles per Gallon'},\n","                 trendline_color_override='red')\n","fig.show()\n","\n","# 3. Boxplots\n","fig = px.box(mtcars, x='cyl', y='mpg', title='MPG Distribution by Number of Cylinders',\n","             color='cyl', color_discrete_sequence=px.colors.qualitative.Set2)\n","fig.update_layout(xaxis_title='Cylinders', yaxis_title='Miles Per Gallon (mpg)', showlegend=False)\n","fig.show()\n","\n","fig = px.box(mtcars, x='gear', y='wt', title='Weight Distribution by Number of Gears',\n","             color='gear', color_discrete_sequence=px.colors.qualitative.Set3)\n","fig.update_layout(xaxis_title='Gears', yaxis_title='Weight (1000 lbs)', showlegend=False)\n","fig.show()\n","\n","# Boxplot with Points\n","fig = go.Figure()\n","fig.add_trace(go.Box(x=mtcars['am'], y=mtcars['mpg'], boxpoints='all', jitter=0.3, pointpos=-1.8,\n","                     marker=dict(color='rgb(255, 127, 127)'), line=dict(color='rgb(255, 127, 127)'),\n","                     name='MPG by Transmission'))\n","fig.update_layout(title='MPG Distribution by Transmission Type',\n","                  xaxis_title='Transmission (0 = Automatic, 1 = Manual)',\n","                  yaxis_title='Miles Per Gallon (mpg)', showlegend=False)\n","fig.update_traces(boxmean=True)  # Show mean line\n","fig.show()\n","\n","# Pair Plot (Scatter Matrix)\n","fig = px.scatter_matrix(mtcars, dimensions=['mpg', 'wt', 'hp', 'cyl'], color='gear',\n","                        title='Pairwise Relationships (Colored by Gear)',\n","                        color_continuous_scale='viridis', height=800)\n","fig.update_traces(diagonal_visible=False)  # Hide diagonal histograms (optional: keep with histogram=True)\n","fig.show()\n","\n","# Correlation matrix\n","corr = mtcars.corr()\n","fig = px.imshow(corr, text_auto=True, title='Feature Correlation Matrix', color_continuous_scale='RdBu', height=600)\n","fig.show()\n","\n","# MPG distribution\n","fig = go.Figure()\n","fig.add_trace(go.Histogram(x=mtcars['mpg'], nbinsx=10, marker=dict(color='skyblue', line=dict(color='black', width=1))))\n","fig.update_layout(title='Distribution of MPG', xaxis_title='mpg', yaxis_title='Frequency', bargap=0.1)\n","fig.show()\n","\n","# Weight vs MPG with gear coloring\n","fig = go.Figure()\n","fig.add_trace(go.Scatter(x=mtcars['wt'], y=mtcars['mpg'], mode='markers',\n","                        marker=dict(size=10, color=mtcars['gear'], colorscale='Viridis', showscale=True),\n","                        text=mtcars.index))\n","fig.update_layout(title='Weight vs MPG (colored by Gear)', xaxis_title='Weight (wt)',\n","                  yaxis_title='Miles Per Gallon (mpg)', coloraxis_colorbar_title='Number of Gears')\n","fig.show()\n","\n","# Summary stats\n","print(\"\\nSummary Statistics:\")\n","print(mtcars.describe())"]},{"cell_type":"markdown","metadata":{"id":"d8PPTXK3JILX"},"source":["- **Key Points:**\n","  - **Gear Histogram:** Most cars have 3 or 4 gears, few have 5. Does gear affect `mpg`?\n","  - **Weight vs MPG Scatter:** Lighter cars (low `wt`) tend to have higher `mpg`. The trendline confirms a negative slope.\n","  - **Correlation Matrix:** `wt` (-0.87), `cyl` (-0.85), and `hp` (-0.78) have strong negative correlations with `mpg`. `gear` (0.48) is positive but weaker.\n","  - **MPG Histogram:** MPG ranges from 10 to 34, with a peak around 15–20.\n","  - **Gear-Colored Scatter:** Cars with 5 gears (yellow) are lighter and have higher `mpg`.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"KtN3dqz1JILY"},"source":["#### **3. Mathematical Theory of Linear Regression**\n","**Goal:** Explain the math behind the model.\n","\n","- **Formulation:**\n","  - Linear regression predicts `mpg` (y) as a weighted sum of features (X) plus a bias.\n","  - Equation:  \n","    $\n","    y = w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots + w_n \\cdot x_n + b\n","    $\n","    - $ y $: Predicted `mpg`\n","    - $ x_1, x_2, \\dots, x_n $: Features (`wt`, `hp`, `cyl`, `gear`)\n","    - $ w_1, w_2, \\dots, w_n $: Weights (learned)\n","    - $ b $: Bias (learned)\n","\n","- **Loss Function:**\n","  - We minimize the Mean Squared Error (MSE) to find the best $ w $ and $ b $:\n","\n","    $\n","    \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n","    $\n","    - $ m $: Number of samples\n","    - $ y_i $: Actual `mpg`\n","    - $ \\hat{y}_i $: Predicted `mpg`\n","\n","- **Optimization:**\n","  - We use Stochastic Gradient Descent (SGD) to update weights:\n","\n","    $\n","    w_j \\leftarrow w_j - \\eta \\cdot \\frac{\\partial \\text{MSE}}{\\partial w_j}\n","    $\n","    \n","    $\n","    b \\leftarrow b - \\eta \\cdot \\frac{\\partial \\text{MSE}}{\\partial b}\n","    $\n","    - $ \\eta $: Learning rate (0.01 in our case)\n","    - Gradients are computed automatically by PyTorch via backpropagation.\n","\n","- **Note:**\n","  - Draw the equation on a board: $ \\text{mpg} = w_1 \\cdot \\text{wt} + w_2 \\cdot \\text{hp} + w_3 \\cdot \\text{cyl} + w_4 \\cdot \\text{gear} + b $.\n","  - Weights tell us how much each feature impacts `mpg`. Negative $ w $ means higher feature values lower `mpg`.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"fbbHlB8dJILY"},"source":["#### **4. Building the Linear Model in PyTorch**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YTnZyXmmJILY"},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Prepare data\n","features = ['wt', 'hp', 'cyl', 'gear']\n","X = mtcars[features].values\n","y = mtcars['mpg'].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","X_train = torch.FloatTensor(X_train)\n","X_test = torch.FloatTensor(X_test)\n","y_train = torch.FloatTensor(y_train).view(-1, 1)\n","y_test = torch.FloatTensor(y_test).view(-1, 1)\n","\n","# Define model\n","class LinearModel(nn.Module):\n","    def __init__(self, input_size):\n","        super(LinearModel, self).__init__()\n","        self.linear = nn.Linear(input_size, 1)\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","input_size = X_train.shape[1]\n","model = LinearModel(input_size)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","\n","# Train\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    outputs = model(X_train)\n","    loss = criterion(outputs, y_train)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    if (epoch + 1) % 100 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","# Evaluate\n","model.eval()\n","with torch.no_grad():\n","    y_pred = model(X_test)\n","    test_loss = criterion(y_pred, y_test)\n","    print(f'Test Loss: {test_loss.item():.4f}')\n","\n","# Results\n","weights = model.linear.weight.data.numpy()\n","bias = model.linear.bias.data.numpy()\n","print(\"\\nModel Weights:\", weights)\n","print(\"Bias:\", bias)"]},{"cell_type":"markdown","metadata":{"id":"Q1JT7Bu9JILZ"},"source":["- **Explanation:**\n","  - **Data Prep:** We standardize features so `wt` (1.5–5.4) and `hp` (52–335) are on the same scale.\n","  - **Model:** This is $ y = w \\cdot X + b $. `nn.Linear` computes it.\n","  - **Training:** Loss drops from 12.5 to 5.0, meaning the model learns. SGD adjusts $ w $ and $ b $ to minimize MSE.\n","  - **Weights:** Negative weights for `wt` (-2.65), `hp` (-1.43), `cyl` (-1.34) match our EDA—higher values lower `mpg`. Positive `gear` (0.52) suggests more gears increase `mpg`.\n","  - **Bias:** 20.17 is the baseline `mpg` when all features are zero (after scaling).\n","\n","- **Graph:**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KcmVARD0JILZ"},"outputs":[],"source":["fig = go.Figure()\n","fig.add_trace(go.Scatter(x=y_test.flatten(), y=y_pred.flatten(), mode='markers', marker=dict(size=10, color='blue'), name='Predictions'))\n","fig.add_trace(go.Scatter(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()], mode='lines', line=dict(color='red', dash='dash'), name='Perfect Prediction'))\n","fig.update_layout(title='Predicted vs Actual MPG', xaxis_title='Actual MPG', yaxis_title='Predicted MPG', showlegend=True)\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"FeyJROjlJILZ"},"source":["  \n","  - Points near the red line are good predictions. Some scatter shows errors.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"axktVbVeJILZ"},"source":["#### **5. Extending to a Deep Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PK6zmfYSJILa"},"outputs":[],"source":["class DeepLinearModel(nn.Module):\n","    def __init__(self, input_size):\n","        super(DeepLinearModel, self).__init__()\n","        self.layer1 = nn.Linear(input_size, 10)\n","        self.relu = nn.ReLU()\n","        self.layer2 = nn.Linear(10, 1)\n","    def forward(self, x):\n","        x = self.relu(self.layer1(x))\n","        x = self.layer2(x)\n","        return x\n","\n","model = DeepLinearModel(input_size)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","losses = []\n","for epoch in range(num_epochs):\n","    outputs = model(X_train)\n","    loss = criterion(outputs, y_train)\n","    losses.append(loss.item())\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    if (epoch + 1) % 100 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","model.eval()\n","with torch.no_grad():\n","    y_pred = model(X_test)\n","    test_loss = criterion(y_pred, y_test)\n","    print(f'Test Loss: {test_loss.item():.4f}')"]},{"cell_type":"markdown","metadata":{"id":"U1Qhy5sIJILa"},"source":["- **Graph:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_z4XlaTJILa"},"outputs":[],"source":["fig = go.Figure()\n","fig.add_trace(go.Scatter(x=np.arange(1, num_epochs + 1), y=losses, mode='lines', line=dict(color='blue'), name='Training Loss'))\n","fig.update_layout(title='Training Loss Over Epochs', xaxis_title='Epoch', yaxis_title='Loss')\n","fig.show()\n","\n","fig = go.Figure()\n","fig.add_trace(go.Scatter(x=y_test.flatten(), y=y_pred.flatten(), mode='markers', marker=dict(size=10, color='blue'), name='Predictions'))\n","fig.add_trace(go.Scatter(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()], mode='lines', line=dict(color='red', dash='dash'), name='Perfect Prediction'))\n","fig.update_layout(title='Predicted vs Actual MPG (Deep Model)', xaxis_title='Actual MPG', yaxis_title='Predicted MPG')\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"b51K4M59JILb"},"source":["- **Explanation:**\n","  - We add a hidden layer with 10 neurons and ReLU to capture non-linear patterns. Loss drops to 2.4, better than the 5.0 from linear model.\n","  - Test loss (7.06) is still higher than training, suggesting some overfitting.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"pSY15UHQJILb"},"source":["#### **6. Discussion**"]},{"cell_type":"markdown","metadata":{"id":"0rg4bN_wJILb"},"source":["- **Model Comparison:**\n","  - Linear: Train Loss = 4.9996, Test Loss = 7.7062\n","  - Deep: Train Loss = 2.4012, Test Loss = 7.0613\n","  - The deep model fits training data better (lower loss), but both struggle on test data. Why? Small dataset (32 cars) and overfitting.\n","\n","- **Weights Insight:**\n","  - Negative `wt` weight (-2.65) aligns with our scatter plot. Lighter cars have higher `mpg`.\n","\n","- **Questions:**\n","  - What features seem most important? How could we improve the model? (Hint: More data, regularization, feature selection.)\n","\n","---\n","\n","### Mathematical Recap Handout\n","- **Model:** $ \\hat{y} = W \\cdot X + b $\n","- **Loss:** $ \\text{MSE} = \\frac{1}{m} \\sum (y - \\hat{y})^2 $\n","- **Gradient Update:** $ w_j \\leftarrow w_j - \\eta \\cdot \\frac{\\partial \\text{MSE}}{\\partial w_j} $\n"]},{"cell_type":"markdown","metadata":{"id":"8H-_QgmlJILb"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Y4UI26O0JILb"},"source":["#### **REFERENCES**\n","\n","Krasser, Roland (2024). Explore mtcars\n","\n","[1] Krasser, R. (2024). Explore mtcars. Cran R-project [https://cran.r-project.org/web/packages/explore/vignettes/explore-mtcars.html](https://cran.r-project.org/web/packages/explore/vignettes/explore-mtcars.html)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}